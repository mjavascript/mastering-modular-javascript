[[shaping-internals]]
== Shaping Internals

Thus far we've addressed modular design and API design concerns from a high level perspective, but avoided plunging into the deep end of implementation details. In contrast, this chapter is devoted to advice and concrete actions we can take to improve the quality of our component implementations. We'll discuss complexity, ways to remediate it, the perils of state, and how to better leverage data structures.

=== 4.1 Internal Complexity

Every piece of code we write is a source of internal complexity, with the potential to become a large pain point for our codebase as a whole. That said, most bits of code are relatively harmless when compared to the entire corpus of our codebase, and trying to proof our code against complexity is a sure way of increasing complexity for no observable benefit. The question is, then, how do we identify the small problems before they grow into a serious threat to the maintainability of our project?

Making a conscious effort to track pieces of code that we haven't changed or interacted with in a while, and identifying if they're simple enough to understand can help us determine whether refactoring may be in order. We could perhaps set a rule whereby team members should watch out for garden paths in the codebase and fix them as they are making changes in the same functional area as the affected code. When we track complexity methodically, often, and across the entire team that's responsible for a codebase, we can expect to see many small but cumulative gains in our battle against complexity.

==== 4.1.1 Containing Nested Complexity

In JavaScript, deep nesting is one of the clearest signs of complexity. Understanding the code at any given nesting level involves understanding how the flow arrives there, the state at every level in scope, how the flow might break out of the level, and which other flows might lead to the same level. Granted, we don't always need to keep all this derived information in our memory. The problem is that, when we do, we might have to spend quite a few minutes reading and understanding the code, deriving such information, and otherwise not fixing the bug or implementing the feature that we had set out to resolve in the first place.

Nesting is the underlying source of complexity in patterns such as "Callback Hell", or "Promise Hell", where callbacks are nested on top of one another. The complexity has little to do with spacing, although when taken to the extreme it does make code harder to read. Instead, the complexity exists at the seams, where we need to fully understand the context in order to go deep into the callback chain and make fixes or improvements. An insidious variant of callback hell is the one where we have logic in every nesting level. This variant is coincidentally the one we can observe most often in real applications: we rarely have callbacks as depicted in the bit of code below, partly because it's immediately obvious that something is wrong. We should probably either change the API so that we get everything we need at once, or we could leverage a small library that takes care of the flow while eliminating the deep nesting we'd otherwise have in our own code.

[source,javascript]
----
getProducts(products => {
  getProductPrices(products, prices => {
    getProductDetails({ products, prices }, details => {
      // …
    })
  })
})
----

When we have synchronous logic intermixed with asynchronous callbacks, things get more challenging. The problem here is, almost always, a coupling of concerns. When a program has a series of nested callbacks that also include logic in between, it can be a sign that we're mixing flow control concerns with business concerns. In other words, our program would be in a better place if we kept the flow separate from the business logic. By splitting the code that purely determines the flow from the rest, we can better isolate our logic into its individual components. The flow, in turn, also becomes more clear because it's now spelled out in plain sight instead of interleaved with business concerns.

Suppose that each nesting level in a series of callbacks contains about 50 lines of code. Each function in the series needs to reference zero, one, or more variables in its parent scope. If it needs zero references to its immediate parent scope, we can safely move it up to the same scope as its parent. We can repeat this process until the function is in the highest possible level where it can, given the variables it has to reference. When functions reference at least one variable from the parent scope, we could opt to leave them unchanged or to pass those references as parameters so that we can keep on decoupling the functions.

As we move logic into their own functions and flatten the callback chain, we'll be left with the bare flow of operations being separate from the operations themselves. Libraries like `contra` can help manage the flow itself while user code worries about business logic.

==== 4.1.2 Feature Entanglement and Tight Coupling

As a module becomes larger, it also gets easier to mistakenly collapse distinct features together by interleaving their code in such a way that it is hard to reuse each feature independently, debug and maintain them, or otherwise extricate the features from one another.

For example, if we have a feature for notifying subscribers and a feature to send notifications, we could strive to keep the features apart by clearly defining how notifications can be constructed and handed off to a different service which then sends those notifications. That way, subscriber notifications can be sent through the notification service, but given the clear separation we won't be letting subscriber-specific notions to get in the way of sending other kinds of notifications to our customers.

One way of reducing the risk of entanglement would be to design features upfront, being particularly on the lookout about concerns that could be componentized or otherwise clearly delineated. By doing a little work before sitting down to write code, we might avert the risks of tight coupling.

Being alert when reading old code can also be key in identifying what was previously a well-contained module that evolved to cover a broad range of concerns. We can then, over time, break apart these concerns into individual modules or better-isolated functions so that each concern is easier to maintain and understand separately.

Instead of trying to build a large feature all at once, it could come in handy to build it from the inside out, keeping each stage of the process in functions that live at the same level instead of being deeply nested. Doing this methodically will lead to better decoupling, as we'll move away from monolithic structures and towards a more modular approach, where functions have smaller scopes and take what they need in the form of parameters.

When we'd have to repeat ourselves by passing a lot of scope variables as function parameters just to avoid nested functions, a light degree of nesting is desirable to avoid this repetition. In key functional boundaries where our concerns go from "gather model details" to "render HTML page" to "print HTML page to PDF", nesting will invariably lead to coupling and less reusability, which is why repeating ourselves a little bit may be warranted in these cases.

==== 4.1.3 Frameworks: the Good the Bad and the Ugly

Conventions are useful because they allow for better self-direction amongst developers, without causing lagoons of inconsistency to spread across our codebase as fate would have it were we to allow a team of developers too much freedom without sound design direction and conventions that dictatehow different portions of an application should be shaped. A large number of conventions might hinder productivity, especially if some of our conventions appeared to work as if by magic.

When it comes to conventions, frameworks are a special case. Frameworks are packed to the brim with conventions and best practices. Some of them live in the library and tooling ecosystem around the framework, while many live in the shape our code takes when we rely on said framework. Upon adopting a framework, we're buying into its conventions and practices. Most modern JavaScript frameworks offer ways of breaking our application into small chunks, regardless of whether the framework is for the client or server.

Express has middleware and routes, Angular has directives, services, and controllers, React has components, and so on and so forth. These conventions and abstractions are tremendously helpful to keep complexity in check while building an application. As our components grow larger, regardless of the abstraction or framework of choice, things will get more complicated. At this moment we usually can refactor our code into smaller components that are then wrapped with larger ones, preserving separation of concerns and keeping complexity on a short leash.

Eventually, we'll come into requirements that don't exactly fit the mold proposed by our framework of choice. Generally, this means the required functionality belongs on a separate layer. For example, Express in Node.js is a framework concerned with handling HTTP requests and serving responses. If one of our API endpoints needs to result in an email being sent, we could embed email-sending logic in the controller for that API endpoint. However, if an API endpoint controller is already concerned with, say, publishing blog posts, then it would be hardly right to embed email-sending logic on that same controller, since it's a different concern entirely. Instead, what we could do is create a `subscribers` service component, with functionality such as `subscribe` which adds a subscriber after verifying their email, and `notify` which takes care of sending the emails. Taking this idea further still, perhaps most of the work in `subscribers.notify` should occur via yet another service component called `emails`, which takes care of properly configuring our email sending capability, and also has functionality to turn would-be emails into plain `console.log` statements for quick access to the contents of the emails during debug sessions.

Having clearly defined layers is paramount to the design of effective and maintainable applications once we're past the prototyping stages. Layers can be made up of components which follow the conventions proposed by the frameworks we use, or they can be self-imposed like the service layer we discussed in the previous paragraph. Using layers, and as long as we favor function parameters over scope for context-passing, we can introduce horizonal scaling by placing several orthogonal components alongside each other, without letting them run into each others' concerns.

=== 4.2 Refactoring Complex Code

Code is ever-evolving, and we'll almost invariably end up with large projects that are not always the easiest to maintain. While we'll reserve the following couple of sections for practical recommendations to reduce complexity at an architectural level, this section focuses on reducing complexity in portions of an application that are already complex.

==== 4.2.1 Embracing Variables over Clever Code

Complex code is predominantly shorter than it should be, and often deceitfully so. An expression that might have involved 5 to 10 short lines of code usually ends up being represented in 1 or 2 clever lines of code. The problem with clever code is that we need to expend time and energy to read it whenever it's intent is not clear on our mind, which is only the case when we first write said code or right after spending considerable time analyzing it.

One of the underlying issues that can be identified when reading complex code is that it uses few variables. In the dawn of programming, memory resources were scarce and thus programmers had to optimize allocation and this often meant reusing variables and using fewer of them. In modern systems, we don't have the need to treat memory as a sacred, precious, and limited resource. Instead, we can focus on making programs readable to both our future selves and fellow developers.

Readability is better served by an abundance of properly named variables or functions than by sparsity. Consider the following example, part of a larger routine, where a program ensures that the user is currently logged in with a valid session, and otherwise redirects them to a login page.

[source,javascript]
----
if (
  auth !== undefined &&
  auth.token !== undefined &&
  auth.expires > Date.now()
) {
  // we have a valid token that hasn't expired yet
  return
}
----

As the routine becomes larger, we collect `if` statements with non-obvious or complicated clauses, such as the reason why we're checking `auth` has a `token` value if we're not doing anything with it here. The solution is usually to add a comment explaining the reason why this check exists. In this case, the comment tells us this is a valid token that hasn't expired. We could turn that comment into code, and simplify the `if` statement in the process, by creating a small function that breaks down the conditional, as shown next.

[source,javascript]
----
function hasValidToken(auth) {
  if (auth === undefined || auth.token === undefined) {
    return false
  }
  const hasNotExpiredYet = auth.expires > Date.now()
  return hasNotExpiredYet
}
----

We can now turn our `if` statement plus comment into a function call, as shown in the following bit of code. Certainly, the totality of our refactored code is a bit longer, but now it's self-descriptive. Code that describes what it does in the process of doing it doesn't require as many comments, and that's important because comments can become easily outdated. Moreover, we've extracted the long conditional in the `if` statement to a function, which keeps us more focused while parsing the codebase. If every condition or task was inline, we'd have to understand everything in order to understand how a program works. When we offload tasks and conditions to other functions, we're letting the reader know they can trust `hasValidToken` to check for validity of the `auth` object, and the conditional becomes a lot easier to digest.

[source,javascript]
----
if (hasValidToken(auth)) {
  return
}
----

We could've used more variables without creating a function, inlining the computation of `hasValidToken` right before the `if` check. A crucial difference between the function-based refactor and the inlining solution is that we used a short-circuiting `return` statement to preemptively bail when we already knew the token was invalidfootnote:[In the example, we immediately return `false` when the token isn't present.], however we can't use `return` statements to bail from the snippet of code that computes `hasValidToken` in the following piece of code without coupling its computation to knowledge about what the routine should return for failure cases. As a result, our only options are tightly coupling the inline subroutine to its containing function, or using a logical or ternary operator in the intermediate steps of the inlined computation.

[source,javascript]
----
const hasToken = auth === undefined || auth.token === undefined
const hasValidToken = hasToken ? auth.expires > Date.now() : false
if (hasValidToken) {
  return
}
----

Both of these options have their downsides. If we couple the return statements with the parent function, we'll need to be careful if we want to replicate the logic elsewhere, as the return statements and possibly their logic will have to adapt as well. If we decide to use ternary operators as a way of short-circuiting, we'll end up with logic that might be as complex as what we originally had in the `if` statement.

Using a function not only avoids these two problems thanks to the ability to `return` intermediate results, but also defers reasoning about its contents until we actually need to understand how tokens are checked for validity.

While moving conditionals to a `function` might sound like a trivial task, this approach is at the heart of modular design. It is by composing small bits of complexity using several additive functions that we can build large applications that are less straining to read. A large pool of mostly trivial functions can add up to a veritable codebase where each bit of code is relatively isolated and easy to understand, provided we trust functions do what their name says they do. In this vein, it is of utmost importance to think long and deep about the name of every function, every variable, and every package, directory, or data structure we conceive.

When used deliberately and extensively, early returns -- sometimes referred to as guard clauses or short-circuits -- can be unparalleled when it comes to making an application as readable as possible. Let's explore this concept in further detail.

==== 4.2.2 Guard Clauses and Branch Flipping

When we have a long branch inside a conditional statement, chances are we're doing something wrong. Pieces of code like the following are commonplace in real world applications, with a long success case branch taking up significant amounts of code while having several `else` branches sprinkled near the end that would log an error, `throw`, return, or otherwise perform a failure handling action.

[source,javascript]
----
if (response) {
  if (!response.errors) {
    // … use `response`
  } else {
    return false
  }
} else {
  return false
}
----

In the example, we're optimizing readability for the success case, while the failure handling is relegated to the very end of our piece of code. There's several problems with this approach. For one, we have to indulge in unnecessary nesting of every success condition, or otherwise put them all in a huge conditional statement. While it's rather easy to understand the success case, things can get tricky when we're trying to debug programs like this, as we need to keep the conditionals in our head the whole time we're reading the program.

A better alternative is to flip the conditionals, placing all failure handling statements near the top. While counterintuitive at first, this approach has several benefits. It reduces nesting and eliminates `else` branches while promoting failure handling to the top of our code, and this has the added benefit that we'll become more aware of error handling and naturally gravitate towards thinking about the failure cases first, a great trait to have when doing application development, where forgetting to handle a failure case might result in an incosistent experience for end users with a hard-to-trace error on top.

[source,javascript]
----
if (!response) {
  return false
}
if (response.errors) {
  return false
}
// … use `response`
----

This early exit approach is often referred to as _guard clauses_, and one of their biggest benefits is that we can learn all the failure cases upon reading the first few lines of a function or piece of code. We're not limited to `return` statements: we could `throw` errors in a promise-based context or in an async function, and in callback chaining contexts we might opt for a `done(error)` callback followed by a `return` statement.

Another benefit of guard clauses is almost implicit: given that they're placed near the top of a function, we have quick access to its parameters, we can better understand how the function validates its inputs, and we can more effectively decide we need to add new guard clauses to improve validation rules.

Guard clauses don't tell the reader everything they need to know that might go wrong when calling a function, but they give them a peek into expected immediate failure cases. Other things that might go wrong lie in the implementation details of the function. Perhaps we use a different service or library to fulfill the bulk of our function's task, and that service or library comes with its own set of nested guard clauses and potential failure cases that will bubble up all the way to our own function's outcome.

==== 4.2.3 An Interdependency Piramid

Writing straightforward code is not all that different from writing other straightforward texts. Texts are often arranged in paragraphs, which are somewhat comparable with functions: we can consider their input to be the reader's knowledge and everything else they've read so far in the text, and the output can be what the reader gets out of the paragraph.

Within a book chapter or any other piece of long-form text, paragraphs are organized in a sequential manner, allowing the reader time to digest each paragraph before they jump onto the next. The logical sequence is very much intentional: without a coherent sequencing, it would be nearly impossible to make sense of a text. Thus, writers optimize for making sure concepts are introduced before they're discussed, providing context to the reader.

Function expressions such as the one in the next snippet won't be assigned to the variable binding until the line containing the assignment is evaluated. Until then, the variable binding exists in the scope, thanks to hoisting, but it is `undefined` until the assignment statement is evaluated.

[source,javascript]
----
double(6) // TypeError: double is not a function
var double = function(x) {
  return x * 2
}
----

Furthermore, if we're dealing with a `let` or `const` binding, then TDZ semantics will produce an error if we reference the binding at all before the variable declaration statement is reached.

[source,javascript]
----
double(6) // TypeError: double is not defined
const double = function(x) {
  return x * 2
}
----

Function declarations like the one in the following snippet, in contrast, are hoisted to the top of the scope. This means we can reference them anywhere in our code.

[source,javascript]
----
double(6) // 12
function double(x) {
  return x * 2
}
----

Now, we mentioned texts are written sequentially, and how the writers avoids surprises by presenting concepts before discussing them. Establishing a context in a program is a different endeavor, however. If we have a module that has the goal of rendering a chart with user engagement statistics, the top of the function should address things the reader already knows, namely the high level flow for what the rendering function is meant to do: analyze the data, construct some data views and model that data into something we can feed into a visualization library that then renders the desired chart.

What we have to avoid is jumping directly into unimportant functions such as a data point label formatter, or the specifics of the data modelling. By keeping only the high level flow near the top, and the specifics towards the end, complex functionality can be designed in such a way that the reader experiences a zoomed out overview of the functionality at first, and as they read the code they uncover the details of how this chart was implemented.

In a concrete sense, this means we should present functions in a codebase in the order that they'll be read by the consumer (a first-in first-out queue), and not in the execution order (a last-in first-out stack). While computers do as they're told and dig ever deeper into the flow, executing the most deeply nested routines before jumping out of a series of subroutines and executing the next line, this is an unfruitful way for humans to read a codebase, given we're ill-suited to keeping all that state in our heads.

Perhaps a more specific analogy for this kind of spiraling approach can be found in newspaper articles, where the author typically offers a title that describes an event at the highest possible level, and then follows up with a lead paragraph that summarizes what happened, again at a high level. The body of the article starts also at a high level, carefully avoiding to spook the reader with too many details. It is only midway through the article that we'll start finding details about the event which, aided by the context set forth in the beginning of the article, can give us a complete picture of what transpired.

Given the stack-based nature of programming, it's not that easy to naturally approach programs as if they were newspaper articles. We can, however, defer execution of implementation details to other functions or subroutines, and thanks to hoisting, we can place those subroutines after their higher level counterparts. In doing so, we're organizing our programs in a way that invites readers in, shows them a few high level hints, and then gradually unveils the spooky details of how a feature is implemented.

==== 4.2.3 Extracting Functions

Deliberate, pyramidal structures where we deal with higher level concerns near the top and switch to more specific problems as we go deeper into the inner workings of a system works wonders in keeping complexity on a tight leash. Such structures are particularly powerful because they break up complex items into their own individual units near the flat bottom of the system, avoiding a complicated interweaving of concerns that are fuzzied together, becoming undistinguishable from one another over time.

Pushing anything that gets in the way of the current flow to the bottom of a function is an effective way of streamlining readability. As an example, consider the case where we have a non-trivial mapper inline in the heart of a function. In the following code snippet we're mapping the users into user models, as we often need to do when we prepare JSON responses for API calls.

[source,javascript]
----
function getUserModels(done) {
  findUsers((err, users) => {
    if (err) {
      done(err)
      return
    }

    const models = users.map(user => {
      const { name, email } = user
      const model = { name, email }
      if (user.type.includes('admin')) {
        model.admin = true
      }
      return model
    })

    done(null, models)
  })
}
----

Now compare that code to the following bit of code, where we extracted the mapping function and shoved it out of the way. Given the mapping function doesn't need any of the scope from `getUserModels`, we can pull it out of that scope entirely, without the need to place `toUserModel` at the bottom of the `getUserModels` function. This means we can now also reuse `toUserModel` in other routines, we don't have to wonder whether the function actually depends on any of the contaning scope's context anymore, and `getUserModels` is now focused on the higher level flow where we find users, map them to their models, and return them.

[source,javascript]
----
function getUserModels(done) {
  findUsers((err, users) => {
    if (err) {
      done(err)
      return
    }

    const models = users.map(toUserModel)

    done(null, models)
  })
}

function toUserModel(user) {
  const { name, email } = user
  const model = { name, email }
  if (user.type.includes('admin')) {
    model.admin = true
  }
  return model
}
----

Furthermore, if there were additional work to be done between the mapping and the callback, it could also be moved into another small function that wouldn't get in the way of our higher level `getUserModels` function.

A similar case occurs when we have a variable that gets defined based on a condition, as shown in the next snippet. Bits of code like this can distract the reader away from the core purpose of a function, to the point where they're often ignored or glossed over.

[source,javascript]
----
// …
let website = null
if (user.details) {
  website = user.details.website
} else if (user.website) {
  website = user.website
}
// …
----

It's best to refactor this kind of assignments into a function, like the one shown next. Note how we've included a `user` parameter so that we can push the function out of the scope chain where we've originally defined the user object, and at the same time went from a `let` binding to a `const` binding. When reading this piece of code later down the line, the benefit of `const` is that we'll know the binding won't change, as opposed to `let` with which we can't be certain bindings won't change over time, adding to the pile of things the reader should be watching out for when trying to understand the algorithm.

[source,javascript]
----
// …
const website = getUserWebsite(user)
// …

function getUserWebsite(user) {
  if (user.details) {
    return user.details.website
  }
  if (user.website) {
    return user.website
  }
  return null
}
----

When we want to name an aspect of a routine without adding a comment, we could create a function to host that functionality. Doing so doesn't just give a name to what the algorithm is doing, but it also allows us to push that code out of the way, leaving behind only the high level description of what's going to happen.

==== 4.2.4 Flattening Nested Callbacks

.. tips/tricks to deal w async flows







==== 4.2.5 Factoring Similar Tasks

.. (such as when we do `createTaskFor(articles)` and `createTaskFor(weeklies)`), or maybe passing a few things but preserving internal task flow overall

==== 4.2.6 Disentangling Headphone Wires

.. a bit of a rehash of 4.2.2 where we visit how we can decouple unrelated functionality by creating focused functions under a shared scope.

==== 4.2.7 Slicing Large Functions

When all else fails, consider breaking what would otherwise inevitably be a large function into smaller functions.

.. how these can be organized by splitting the functionality by steps, aspects, or any other logical grouping we can come up with


=== 4.3 State as Entropy

..

==== 4.3.1 It's Complicated

section on how state is terrible and the more state there is the less predictable our code becomes.

==== 4.3.2 Eliminating Incidental State

.. where possible always recompute state from data rather than rely on state

==== 4.3.3 Containing State

.. when we must have state, keep it as constrained as possible

==== 4.3.4 Leveraging Immutability

..

=== 4.4 Data Structures are King

.. a section on how data structures make or break an application, and why data-driven is better than state or logic driven

==== 4.4.1 Isolating Data

.. Keeping data separate from methods that modify or access said data can help reduce complexity. When data is not cluttered with functionality, it becomes easier to read, understand, and serialize.

==== 4.4.2 Restricting Domain Logic

.. We should strive to keep code that knows about a particular data structure or set of structures contained in as few modules as possible. Should the data structures or the logic around them require changes, the ripple effects of those changes can be devastating if domain logic is spread across the codebase.

==== 4.4.3 Choosing Data Over Code

.. How the right data structures can make our code much easier to write and read. It might be worth mapping data into something that's amenable to the task at hand, so that the algorithm is simplified by making the data easier to consume.

==== 4.4.4 Reducing, Mapping, Filtering, and Sorting Data

..




---

most of this chapter should be javascript code programming best practices:
look up code and figure out things you did. compare to code other people wrote and identify problems, explain.

---
